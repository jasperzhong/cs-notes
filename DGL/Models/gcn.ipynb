{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d9b3869-0922-4090-aca3-788b62eb9dfd",
   "metadata": {},
   "source": [
    "## Graph Convolution Network (GCN)\n",
    "\n",
    "Paper: [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/pdf/1609.02907.pdf) (ICLR 2017)\n",
    "\n",
    "\n",
    "**Propagation Rule**\n",
    "\n",
    "$$H^{(l+1)} = \\sigma \\left(\\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}} H^{(l)} W^{(l)} \\right) ,$$\n",
    "\n",
    "with $\\tilde{A} = A + I$, where $A$ is the adjacency matrix, $I$ is the identity matrix, and $\\tilde{D}$ is the diagonal node degree matrix of $\\tilde{A}$. $W^{(l)}$ is a weight matrix for the $l$-th neural network layer and $\\sigma(\\cdot)$ is a non-linear activation function like the $\\text{ReLU}$.\n",
    "\n",
    "Example of a two-layer GCN: \n",
    "\n",
    "$$Z = f(X, A) = \\text{softmax}\\left(\\hat{A} \\text{ ReLU}\\left(\\hat{A}XW^{(0)} \\right) W^{(1)} \\right) ,$$\n",
    "\n",
    "where $\\hat{A} = \\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}}$, $W^{(0)}$ and $W^{(1)}$ are weight matrices.\n",
    "\n",
    "**Message Passing Perspective**\n",
    "\n",
    "|Notion | Meaning | \n",
    "|---|---|\n",
    "|$\\mathcal{G}$ = $(V, E)$ | Input graph |\n",
    "|$x_v$ | Node features for node $v\\in V$|\n",
    "|$h_v$ | Node embedding for node $v\\in V$ |\n",
    "|$\\mathcal{N}(v)$ | Neighbours of node $v\\in V$|\n",
    "\n",
    "Initial:\n",
    "$$h^{(0)}_v = x_v , \\forall v \\in V .$$\n",
    "\n",
    "Aggregate:\n",
    "$$\\hat{h}_v \\leftarrow\\sum_{u\\in \\{\\mathcal{N}(v) \\cup \\{v\\}\\} }\\frac{h^{(l-1)}_u}{\\sqrt{|\\mathcal{N}(u)| |\\mathcal{N}(v)}|} , \\forall v \\in V .$$\n",
    "\n",
    "Update: \n",
    "$$h^{(l)}_v \\leftarrow \\sigma \\left(W^{(l)}\\cdot \\hat{h}_v \\right), \\forall v \\in V.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099136c3-24a2-4b78-a14d-2f4e6e039001",
   "metadata": {},
   "source": [
    "## Reproduce Results\n",
    "\n",
    "|Dataset | Citeseer | Cora | Pubmed | \n",
    "|---| --- | --- | ---|\n",
    "|Original Paper | 70.3 | 81.5 | 79.0 | \n",
    "|Ours | 70.4 | 81.6 | 78.9 |\n",
    "\n",
    "Refererence of implementation: https://github.com/dmlc/dgl/blob/master/examples/pytorch/gcn/gcn_mp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21557965-1d98-4a21-a99c-b9f5383e7c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl \n",
    "from dgl.data import CoraGraphDataset, CiteseerGraphDataset, PubmedGraphDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9bffb9f-7d71-41ae-a31d-5dbee214e899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_func(edges):\n",
    "    return {'m': edges.src['h'] * edges.src['norm']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "903a8f83-9e70-486e-97ce-3446db53f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_func(nodes):\n",
    "    return {'h': torch.sum(nodes.mailbox['m'], 1) * nodes.data['norm']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1038ec0e-661b-4011-a90d-12aecd814ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, dropout=0.5):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats, bias=False)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = self.dropout(h)\n",
    "            g.update_all(message_func=message_func, reduce_func=reduce_func)\n",
    "            return self.linear(g.ndata['h'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18deac93-9657-4332-86b0-5b123c916ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A two-layer GCN as described in the paper\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes, dropout=0.5):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNLayer(in_feats, h_feats, dropout=dropout)\n",
    "        self.conv2 = GCNLayer(h_feats, num_classes, dropout=dropout)\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = self.conv1(g, h)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfaa5522-16ab-4cb7-a1dc-382bbaa82a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the same configurations as the paper's\n",
    "dropout = 0.5\n",
    "wd = 5e-4\n",
    "hidden_size = 16\n",
    "lr = 0.01\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c603066a-12aa-46f4-8458-d217606c3a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, g, features, labels, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(g, features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb901020-5e70-4371-870f-fa1716de7bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(dataset):\n",
    "    g = dataset[0]\n",
    "    features = g.ndata['feat']\n",
    "    labels = g.ndata['label']\n",
    "    train_mask = g.ndata['train_mask']\n",
    "    val_mask = g.ndata['val_mask']\n",
    "    test_mask = g.ndata['test_mask']\n",
    "    in_feats = features.shape[1]\n",
    "    n_classes = dataset.num_classes\n",
    "    n_edges = g.number_of_edges()\n",
    "    \n",
    "    # add self loop\n",
    "    g = dgl.remove_self_loop(g)\n",
    "    g = dgl.add_self_loop(g)\n",
    "    n_edges = g.number_of_edges()\n",
    "\n",
    "    # normalization\n",
    "    degs = g.in_degrees().float()\n",
    "    norm = torch.pow(degs, -0.5)\n",
    "    norm[torch.isinf(norm)] = 0\n",
    "    g.ndata['norm'] = norm.unsqueeze(1)\n",
    "    \n",
    "    model = GCN(in_feats, hidden_size, n_classes, dropout)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    early_stopping_cnt = 0\n",
    "    best_val = 100 # large enough\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        # forward\n",
    "        logits = model(g, features)\n",
    "        loss = loss_fn(logits[train_mask], labels[train_mask])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        end = time.time()\n",
    "        logits = model(g, features)\n",
    "        val_loss = loss_fn(logits[val_mask], labels[val_mask])\n",
    "        \n",
    "        \n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            early_stopping_cnt = 0\n",
    "        else:\n",
    "            early_stopping_cnt += 1\n",
    "            if early_stopping_cnt == 10:\n",
    "                print(\"Early stopping (val loss does not decrease for 10 consecutive epochs).\")\n",
    "                break\n",
    "        \n",
    "        print(\"Epoch {:03d} | Time(s) {:.4f} | Train Loss {:.4f} | Val Loss {:.4f} | \".format(epoch, end - start, loss.item(), val_loss.item()))\n",
    "    \n",
    "    acc = evaluate(model, g, features, labels, test_mask)\n",
    "    print(\"Test Accuracy {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d28dbc36-8233-4c87-89c1-fd61889d5679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 3327\n",
      "  NumEdges: 9228\n",
      "  NumFeats: 3703\n",
      "  NumClasses: 6\n",
      "  NumTrainingSamples: 120\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n",
      "Epoch 000 | Time(s) 0.6588 | Train Loss 1.7917 | Val Loss 1.7910 | \n",
      "Epoch 001 | Time(s) 0.5917 | Train Loss 1.7896 | Val Loss 1.7896 | \n",
      "Epoch 002 | Time(s) 0.7696 | Train Loss 1.7863 | Val Loss 1.7879 | \n",
      "Epoch 003 | Time(s) 0.7181 | Train Loss 1.7829 | Val Loss 1.7864 | \n",
      "Epoch 004 | Time(s) 0.6971 | Train Loss 1.7784 | Val Loss 1.7850 | \n",
      "Epoch 005 | Time(s) 0.6932 | Train Loss 1.7750 | Val Loss 1.7831 | \n",
      "Epoch 006 | Time(s) 0.7345 | Train Loss 1.7689 | Val Loss 1.7817 | \n",
      "Epoch 007 | Time(s) 0.6775 | Train Loss 1.7664 | Val Loss 1.7792 | \n",
      "Epoch 008 | Time(s) 0.7664 | Train Loss 1.7596 | Val Loss 1.7774 | \n",
      "Epoch 009 | Time(s) 0.7022 | Train Loss 1.7533 | Val Loss 1.7762 | \n",
      "Epoch 010 | Time(s) 0.7091 | Train Loss 1.7469 | Val Loss 1.7738 | \n",
      "Epoch 011 | Time(s) 0.7565 | Train Loss 1.7414 | Val Loss 1.7717 | \n",
      "Epoch 012 | Time(s) 0.6959 | Train Loss 1.7382 | Val Loss 1.7691 | \n",
      "Epoch 013 | Time(s) 0.6050 | Train Loss 1.7318 | Val Loss 1.7680 | \n",
      "Epoch 014 | Time(s) 0.7448 | Train Loss 1.7266 | Val Loss 1.7642 | \n",
      "Epoch 015 | Time(s) 0.6098 | Train Loss 1.7167 | Val Loss 1.7621 | \n",
      "Epoch 016 | Time(s) 0.6786 | Train Loss 1.7066 | Val Loss 1.7579 | \n",
      "Epoch 017 | Time(s) 0.7124 | Train Loss 1.7011 | Val Loss 1.7532 | \n",
      "Epoch 018 | Time(s) 0.6396 | Train Loss 1.6934 | Val Loss 1.7531 | \n",
      "Epoch 019 | Time(s) 0.6535 | Train Loss 1.6843 | Val Loss 1.7505 | \n",
      "Epoch 020 | Time(s) 0.6507 | Train Loss 1.6750 | Val Loss 1.7468 | \n",
      "Epoch 021 | Time(s) 0.6729 | Train Loss 1.6652 | Val Loss 1.7439 | \n",
      "Epoch 022 | Time(s) 0.6609 | Train Loss 1.6644 | Val Loss 1.7384 | \n",
      "Epoch 023 | Time(s) 0.7208 | Train Loss 1.6577 | Val Loss 1.7358 | \n",
      "Epoch 024 | Time(s) 0.6186 | Train Loss 1.6357 | Val Loss 1.7362 | \n",
      "Epoch 025 | Time(s) 0.7432 | Train Loss 1.6298 | Val Loss 1.7283 | \n",
      "Epoch 026 | Time(s) 0.6698 | Train Loss 1.6084 | Val Loss 1.7280 | \n",
      "Epoch 027 | Time(s) 0.6948 | Train Loss 1.6041 | Val Loss 1.7204 | \n",
      "Epoch 028 | Time(s) 0.6982 | Train Loss 1.5898 | Val Loss 1.7119 | \n",
      "Epoch 029 | Time(s) 0.6976 | Train Loss 1.5794 | Val Loss 1.7117 | \n",
      "Epoch 030 | Time(s) 0.6944 | Train Loss 1.5664 | Val Loss 1.7038 | \n",
      "Epoch 031 | Time(s) 0.6549 | Train Loss 1.5660 | Val Loss 1.6971 | \n",
      "Epoch 032 | Time(s) 0.6802 | Train Loss 1.5526 | Val Loss 1.6938 | \n",
      "Epoch 033 | Time(s) 0.6981 | Train Loss 1.5299 | Val Loss 1.6938 | \n",
      "Epoch 034 | Time(s) 0.5884 | Train Loss 1.5194 | Val Loss 1.6846 | \n",
      "Epoch 035 | Time(s) 0.6031 | Train Loss 1.5156 | Val Loss 1.6841 | \n",
      "Epoch 036 | Time(s) 0.7276 | Train Loss 1.4879 | Val Loss 1.6760 | \n",
      "Epoch 037 | Time(s) 0.6761 | Train Loss 1.4869 | Val Loss 1.6801 | \n",
      "Epoch 038 | Time(s) 0.7687 | Train Loss 1.4774 | Val Loss 1.6768 | \n",
      "Epoch 039 | Time(s) 0.5892 | Train Loss 1.4529 | Val Loss 1.6545 | \n",
      "Epoch 040 | Time(s) 0.7030 | Train Loss 1.4363 | Val Loss 1.6577 | \n",
      "Epoch 041 | Time(s) 0.6937 | Train Loss 1.4705 | Val Loss 1.6564 | \n",
      "Epoch 042 | Time(s) 0.6907 | Train Loss 1.4138 | Val Loss 1.6387 | \n",
      "Epoch 043 | Time(s) 0.7137 | Train Loss 1.4163 | Val Loss 1.6390 | \n",
      "Epoch 044 | Time(s) 0.6032 | Train Loss 1.3616 | Val Loss 1.6356 | \n",
      "Epoch 045 | Time(s) 0.6724 | Train Loss 1.3531 | Val Loss 1.6313 | \n",
      "Epoch 046 | Time(s) 0.7542 | Train Loss 1.3433 | Val Loss 1.6109 | \n",
      "Epoch 047 | Time(s) 0.7468 | Train Loss 1.3312 | Val Loss 1.6023 | \n",
      "Epoch 048 | Time(s) 0.6750 | Train Loss 1.3001 | Val Loss 1.6053 | \n",
      "Epoch 049 | Time(s) 0.7175 | Train Loss 1.2992 | Val Loss 1.6060 | \n",
      "Epoch 050 | Time(s) 0.7170 | Train Loss 1.2909 | Val Loss 1.5836 | \n",
      "Epoch 051 | Time(s) 0.7421 | Train Loss 1.2795 | Val Loss 1.5998 | \n",
      "Epoch 052 | Time(s) 0.7132 | Train Loss 1.2776 | Val Loss 1.5889 | \n",
      "Epoch 053 | Time(s) 0.7003 | Train Loss 1.2635 | Val Loss 1.5742 | \n",
      "Epoch 054 | Time(s) 0.6055 | Train Loss 1.2569 | Val Loss 1.5758 | \n",
      "Epoch 055 | Time(s) 0.6686 | Train Loss 1.2008 | Val Loss 1.5771 | \n",
      "Epoch 056 | Time(s) 0.5921 | Train Loss 1.2204 | Val Loss 1.5404 | \n",
      "Epoch 057 | Time(s) 0.6802 | Train Loss 1.1911 | Val Loss 1.5493 | \n",
      "Epoch 058 | Time(s) 0.6749 | Train Loss 1.2002 | Val Loss 1.5518 | \n",
      "Epoch 059 | Time(s) 0.6479 | Train Loss 1.1191 | Val Loss 1.5409 | \n",
      "Epoch 060 | Time(s) 0.6803 | Train Loss 1.1448 | Val Loss 1.5180 | \n",
      "Epoch 061 | Time(s) 0.6192 | Train Loss 1.1158 | Val Loss 1.5192 | \n",
      "Epoch 062 | Time(s) 0.7004 | Train Loss 1.1576 | Val Loss 1.5291 | \n",
      "Epoch 063 | Time(s) 0.6742 | Train Loss 1.0778 | Val Loss 1.5221 | \n",
      "Epoch 064 | Time(s) 0.6934 | Train Loss 1.0755 | Val Loss 1.4969 | \n",
      "Epoch 065 | Time(s) 0.7151 | Train Loss 1.0727 | Val Loss 1.5155 | \n",
      "Epoch 066 | Time(s) 0.6881 | Train Loss 1.0417 | Val Loss 1.5026 | \n",
      "Epoch 067 | Time(s) 0.6873 | Train Loss 1.0416 | Val Loss 1.5017 | \n",
      "Epoch 068 | Time(s) 0.6748 | Train Loss 1.0429 | Val Loss 1.4936 | \n",
      "Epoch 069 | Time(s) 0.6585 | Train Loss 0.9987 | Val Loss 1.4851 | \n",
      "Epoch 070 | Time(s) 0.6877 | Train Loss 1.0251 | Val Loss 1.4844 | \n",
      "Epoch 071 | Time(s) 0.6625 | Train Loss 1.0247 | Val Loss 1.4766 | \n",
      "Epoch 072 | Time(s) 0.6190 | Train Loss 0.9917 | Val Loss 1.4637 | \n",
      "Epoch 073 | Time(s) 0.6566 | Train Loss 0.9950 | Val Loss 1.4660 | \n",
      "Epoch 074 | Time(s) 0.6385 | Train Loss 0.9685 | Val Loss 1.4546 | \n",
      "Epoch 075 | Time(s) 0.7192 | Train Loss 0.9864 | Val Loss 1.4557 | \n",
      "Epoch 076 | Time(s) 0.6929 | Train Loss 0.9687 | Val Loss 1.4279 | \n",
      "Epoch 077 | Time(s) 0.7280 | Train Loss 0.9118 | Val Loss 1.4435 | \n",
      "Epoch 078 | Time(s) 0.7168 | Train Loss 0.9201 | Val Loss 1.4492 | \n",
      "Epoch 079 | Time(s) 0.6771 | Train Loss 0.9092 | Val Loss 1.4503 | \n",
      "Epoch 080 | Time(s) 0.5951 | Train Loss 0.9474 | Val Loss 1.4180 | \n",
      "Epoch 081 | Time(s) 0.6927 | Train Loss 0.8875 | Val Loss 1.4392 | \n",
      "Epoch 082 | Time(s) 0.6707 | Train Loss 0.8755 | Val Loss 1.4372 | \n",
      "Epoch 083 | Time(s) 0.7393 | Train Loss 0.9173 | Val Loss 1.4383 | \n",
      "Epoch 084 | Time(s) 0.7357 | Train Loss 0.8991 | Val Loss 1.4170 | \n",
      "Epoch 085 | Time(s) 0.7102 | Train Loss 0.8623 | Val Loss 1.4110 | \n",
      "Epoch 086 | Time(s) 0.6355 | Train Loss 0.8529 | Val Loss 1.4112 | \n",
      "Epoch 087 | Time(s) 0.6658 | Train Loss 0.8576 | Val Loss 1.4216 | \n",
      "Epoch 088 | Time(s) 0.6258 | Train Loss 0.8408 | Val Loss 1.4190 | \n",
      "Epoch 089 | Time(s) 0.6723 | Train Loss 0.8211 | Val Loss 1.3999 | \n",
      "Epoch 090 | Time(s) 0.6587 | Train Loss 0.8352 | Val Loss 1.3775 | \n",
      "Epoch 091 | Time(s) 0.6790 | Train Loss 0.7556 | Val Loss 1.3748 | \n",
      "Epoch 092 | Time(s) 0.7275 | Train Loss 0.8334 | Val Loss 1.3981 | \n",
      "Epoch 093 | Time(s) 0.7477 | Train Loss 0.8218 | Val Loss 1.3604 | \n",
      "Epoch 094 | Time(s) 0.6413 | Train Loss 0.7842 | Val Loss 1.3812 | \n",
      "Epoch 095 | Time(s) 0.6836 | Train Loss 0.7655 | Val Loss 1.3810 | \n",
      "Epoch 096 | Time(s) 0.7220 | Train Loss 0.8212 | Val Loss 1.3985 | \n",
      "Epoch 097 | Time(s) 0.7176 | Train Loss 0.8012 | Val Loss 1.4046 | \n",
      "Epoch 098 | Time(s) 0.6890 | Train Loss 0.7870 | Val Loss 1.3791 | \n",
      "Epoch 099 | Time(s) 0.6877 | Train Loss 0.7626 | Val Loss 1.3842 | \n",
      "Epoch 100 | Time(s) 0.6806 | Train Loss 0.7779 | Val Loss 1.3407 | \n",
      "Epoch 101 | Time(s) 0.6922 | Train Loss 0.7948 | Val Loss 1.3566 | \n",
      "Epoch 102 | Time(s) 0.6666 | Train Loss 0.7562 | Val Loss 1.3458 | \n",
      "Epoch 103 | Time(s) 0.6469 | Train Loss 0.7321 | Val Loss 1.3767 | \n",
      "Epoch 104 | Time(s) 0.6710 | Train Loss 0.7755 | Val Loss 1.3917 | \n",
      "Epoch 105 | Time(s) 0.6832 | Train Loss 0.7405 | Val Loss 1.3574 | \n",
      "Epoch 106 | Time(s) 0.6787 | Train Loss 0.7398 | Val Loss 1.3494 | \n",
      "Epoch 107 | Time(s) 0.6964 | Train Loss 0.7355 | Val Loss 1.3534 | \n",
      "Epoch 108 | Time(s) 0.6147 | Train Loss 0.7044 | Val Loss 1.3271 | \n",
      "Epoch 109 | Time(s) 0.6887 | Train Loss 0.6801 | Val Loss 1.3523 | \n",
      "Epoch 110 | Time(s) 0.6972 | Train Loss 0.7198 | Val Loss 1.3519 | \n",
      "Epoch 111 | Time(s) 0.6803 | Train Loss 0.7122 | Val Loss 1.3455 | \n",
      "Epoch 112 | Time(s) 0.6604 | Train Loss 0.7210 | Val Loss 1.3526 | \n",
      "Epoch 113 | Time(s) 0.6043 | Train Loss 0.6720 | Val Loss 1.3025 | \n",
      "Epoch 114 | Time(s) 0.6945 | Train Loss 0.6928 | Val Loss 1.3254 | \n",
      "Epoch 115 | Time(s) 0.7139 | Train Loss 0.6713 | Val Loss 1.3189 | \n",
      "Epoch 116 | Time(s) 0.7053 | Train Loss 0.6634 | Val Loss 1.3327 | \n",
      "Epoch 117 | Time(s) 0.6248 | Train Loss 0.6766 | Val Loss 1.3309 | \n",
      "Epoch 118 | Time(s) 0.6954 | Train Loss 0.6409 | Val Loss 1.3274 | \n",
      "Epoch 119 | Time(s) 0.6879 | Train Loss 0.7067 | Val Loss 1.2967 | \n",
      "Epoch 120 | Time(s) 0.6921 | Train Loss 0.6684 | Val Loss 1.3212 | \n",
      "Epoch 121 | Time(s) 0.6993 | Train Loss 0.6641 | Val Loss 1.3037 | \n",
      "Epoch 122 | Time(s) 0.6786 | Train Loss 0.6485 | Val Loss 1.2980 | \n",
      "Epoch 123 | Time(s) 0.7203 | Train Loss 0.7070 | Val Loss 1.3170 | \n",
      "Epoch 124 | Time(s) 0.7465 | Train Loss 0.6781 | Val Loss 1.3367 | \n",
      "Epoch 125 | Time(s) 0.7040 | Train Loss 0.6335 | Val Loss 1.3085 | \n",
      "Epoch 126 | Time(s) 0.6078 | Train Loss 0.6698 | Val Loss 1.2892 | \n",
      "Epoch 127 | Time(s) 0.6848 | Train Loss 0.6927 | Val Loss 1.2836 | \n",
      "Epoch 128 | Time(s) 0.7148 | Train Loss 0.6599 | Val Loss 1.3358 | \n",
      "Epoch 129 | Time(s) 0.6508 | Train Loss 0.6194 | Val Loss 1.2996 | \n",
      "Epoch 130 | Time(s) 0.6116 | Train Loss 0.6149 | Val Loss 1.2691 | \n",
      "Epoch 131 | Time(s) 0.6285 | Train Loss 0.6010 | Val Loss 1.2738 | \n",
      "Epoch 132 | Time(s) 0.6850 | Train Loss 0.6083 | Val Loss 1.2832 | \n",
      "Epoch 133 | Time(s) 0.6736 | Train Loss 0.6306 | Val Loss 1.2991 | \n",
      "Epoch 134 | Time(s) 0.6416 | Train Loss 0.6274 | Val Loss 1.2680 | \n",
      "Epoch 135 | Time(s) 0.6845 | Train Loss 0.5435 | Val Loss 1.2414 | \n",
      "Epoch 136 | Time(s) 0.7152 | Train Loss 0.5961 | Val Loss 1.2717 | \n",
      "Epoch 137 | Time(s) 0.7191 | Train Loss 0.5739 | Val Loss 1.3022 | \n",
      "Epoch 138 | Time(s) 0.6015 | Train Loss 0.5554 | Val Loss 1.3161 | \n",
      "Epoch 139 | Time(s) 0.6763 | Train Loss 0.5775 | Val Loss 1.2879 | \n",
      "Epoch 140 | Time(s) 0.6886 | Train Loss 0.6261 | Val Loss 1.2873 | \n",
      "Epoch 141 | Time(s) 0.7225 | Train Loss 0.5910 | Val Loss 1.2487 | \n",
      "Epoch 142 | Time(s) 0.6793 | Train Loss 0.5857 | Val Loss 1.2818 | \n",
      "Epoch 143 | Time(s) 0.6052 | Train Loss 0.5993 | Val Loss 1.3294 | \n",
      "Epoch 144 | Time(s) 0.7041 | Train Loss 0.5749 | Val Loss 1.2596 | \n",
      "Early stopping (val loss does not decrease for 10 consecutive epochs).\n",
      "Test Accuracy 0.7040\n"
     ]
    }
   ],
   "source": [
    "dataset = CiteseerGraphDataset()\n",
    "main(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6256f2a5-8971-4a43-a830-eeeeaea34626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n",
      "Epoch 000 | Time(s) 0.4625 | Train Loss 1.9459 | Val Loss 1.9447 | \n",
      "Epoch 001 | Time(s) 0.4397 | Train Loss 1.9437 | Val Loss 1.9427 | \n",
      "Epoch 002 | Time(s) 0.4622 | Train Loss 1.9404 | Val Loss 1.9404 | \n",
      "Epoch 003 | Time(s) 0.4949 | Train Loss 1.9359 | Val Loss 1.9381 | \n",
      "Epoch 004 | Time(s) 0.5053 | Train Loss 1.9322 | Val Loss 1.9364 | \n",
      "Epoch 005 | Time(s) 0.4676 | Train Loss 1.9275 | Val Loss 1.9333 | \n",
      "Epoch 006 | Time(s) 0.4554 | Train Loss 1.9223 | Val Loss 1.9309 | \n",
      "Epoch 007 | Time(s) 0.4697 | Train Loss 1.9190 | Val Loss 1.9269 | \n",
      "Epoch 008 | Time(s) 0.4046 | Train Loss 1.9141 | Val Loss 1.9250 | \n",
      "Epoch 009 | Time(s) 0.3998 | Train Loss 1.9053 | Val Loss 1.9217 | \n",
      "Epoch 010 | Time(s) 0.4422 | Train Loss 1.8983 | Val Loss 1.9191 | \n",
      "Epoch 011 | Time(s) 0.4379 | Train Loss 1.8909 | Val Loss 1.9151 | \n",
      "Epoch 012 | Time(s) 0.4446 | Train Loss 1.8834 | Val Loss 1.9088 | \n",
      "Epoch 013 | Time(s) 0.4677 | Train Loss 1.8735 | Val Loss 1.9069 | \n",
      "Epoch 014 | Time(s) 0.4744 | Train Loss 1.8682 | Val Loss 1.8980 | \n",
      "Epoch 015 | Time(s) 0.4803 | Train Loss 1.8569 | Val Loss 1.8935 | \n",
      "Epoch 016 | Time(s) 0.5095 | Train Loss 1.8508 | Val Loss 1.8923 | \n",
      "Epoch 017 | Time(s) 0.4051 | Train Loss 1.8394 | Val Loss 1.8874 | \n",
      "Epoch 018 | Time(s) 0.4077 | Train Loss 1.8338 | Val Loss 1.8806 | \n",
      "Epoch 019 | Time(s) 0.4731 | Train Loss 1.8202 | Val Loss 1.8702 | \n",
      "Epoch 020 | Time(s) 0.4738 | Train Loss 1.8067 | Val Loss 1.8702 | \n",
      "Epoch 021 | Time(s) 0.4410 | Train Loss 1.7992 | Val Loss 1.8611 | \n",
      "Epoch 022 | Time(s) 0.4293 | Train Loss 1.7868 | Val Loss 1.8537 | \n",
      "Epoch 023 | Time(s) 0.4239 | Train Loss 1.7692 | Val Loss 1.8533 | \n",
      "Epoch 024 | Time(s) 0.4340 | Train Loss 1.7533 | Val Loss 1.8409 | \n",
      "Epoch 025 | Time(s) 0.4452 | Train Loss 1.7433 | Val Loss 1.8275 | \n",
      "Epoch 026 | Time(s) 0.3973 | Train Loss 1.7356 | Val Loss 1.8245 | \n",
      "Epoch 027 | Time(s) 0.4306 | Train Loss 1.7189 | Val Loss 1.8149 | \n",
      "Epoch 028 | Time(s) 0.4613 | Train Loss 1.6936 | Val Loss 1.8025 | \n",
      "Epoch 029 | Time(s) 0.4294 | Train Loss 1.6978 | Val Loss 1.8015 | \n",
      "Epoch 030 | Time(s) 0.4570 | Train Loss 1.6576 | Val Loss 1.7878 | \n",
      "Epoch 031 | Time(s) 0.5382 | Train Loss 1.6584 | Val Loss 1.7834 | \n",
      "Epoch 032 | Time(s) 0.4943 | Train Loss 1.6468 | Val Loss 1.7756 | \n",
      "Epoch 033 | Time(s) 0.5031 | Train Loss 1.6199 | Val Loss 1.7669 | \n",
      "Epoch 034 | Time(s) 0.4931 | Train Loss 1.6156 | Val Loss 1.7514 | \n",
      "Epoch 035 | Time(s) 0.4881 | Train Loss 1.5980 | Val Loss 1.7544 | \n",
      "Epoch 036 | Time(s) 0.4854 | Train Loss 1.5573 | Val Loss 1.7355 | \n",
      "Epoch 037 | Time(s) 0.4614 | Train Loss 1.5714 | Val Loss 1.7261 | \n",
      "Epoch 038 | Time(s) 0.4652 | Train Loss 1.5467 | Val Loss 1.7111 | \n",
      "Epoch 039 | Time(s) 0.4592 | Train Loss 1.5230 | Val Loss 1.7129 | \n",
      "Epoch 040 | Time(s) 0.4016 | Train Loss 1.5040 | Val Loss 1.6856 | \n",
      "Epoch 041 | Time(s) 0.4009 | Train Loss 1.4569 | Val Loss 1.6745 | \n",
      "Epoch 042 | Time(s) 0.4117 | Train Loss 1.4606 | Val Loss 1.6950 | \n",
      "Epoch 043 | Time(s) 0.4584 | Train Loss 1.4245 | Val Loss 1.6561 | \n",
      "Epoch 044 | Time(s) 0.4624 | Train Loss 1.4206 | Val Loss 1.6414 | \n",
      "Epoch 045 | Time(s) 0.4154 | Train Loss 1.3847 | Val Loss 1.6395 | \n",
      "Epoch 046 | Time(s) 0.3996 | Train Loss 1.3808 | Val Loss 1.6296 | \n",
      "Epoch 047 | Time(s) 0.4489 | Train Loss 1.3496 | Val Loss 1.6118 | \n",
      "Epoch 048 | Time(s) 0.4018 | Train Loss 1.3743 | Val Loss 1.5889 | \n",
      "Epoch 049 | Time(s) 0.4110 | Train Loss 1.3130 | Val Loss 1.5729 | \n",
      "Epoch 050 | Time(s) 0.4446 | Train Loss 1.3168 | Val Loss 1.5855 | \n",
      "Epoch 051 | Time(s) 0.4410 | Train Loss 1.2578 | Val Loss 1.5577 | \n",
      "Epoch 052 | Time(s) 0.4680 | Train Loss 1.2512 | Val Loss 1.5398 | \n",
      "Epoch 053 | Time(s) 0.4579 | Train Loss 1.2136 | Val Loss 1.5415 | \n",
      "Epoch 054 | Time(s) 0.4589 | Train Loss 1.2476 | Val Loss 1.5203 | \n",
      "Epoch 055 | Time(s) 0.4110 | Train Loss 1.1995 | Val Loss 1.5252 | \n",
      "Epoch 056 | Time(s) 0.4023 | Train Loss 1.1791 | Val Loss 1.5341 | \n",
      "Epoch 057 | Time(s) 0.4284 | Train Loss 1.2072 | Val Loss 1.4958 | \n",
      "Epoch 058 | Time(s) 0.4558 | Train Loss 1.1285 | Val Loss 1.4954 | \n",
      "Epoch 059 | Time(s) 0.4569 | Train Loss 1.1080 | Val Loss 1.4558 | \n",
      "Epoch 060 | Time(s) 0.4415 | Train Loss 1.0597 | Val Loss 1.4545 | \n",
      "Epoch 061 | Time(s) 0.4803 | Train Loss 1.0887 | Val Loss 1.4498 | \n",
      "Epoch 062 | Time(s) 0.5136 | Train Loss 1.0314 | Val Loss 1.4148 | \n",
      "Epoch 063 | Time(s) 0.4911 | Train Loss 1.0276 | Val Loss 1.4157 | \n",
      "Epoch 064 | Time(s) 0.5012 | Train Loss 1.0296 | Val Loss 1.4045 | \n",
      "Epoch 065 | Time(s) 0.4668 | Train Loss 0.9995 | Val Loss 1.4102 | \n",
      "Epoch 066 | Time(s) 0.4622 | Train Loss 0.9986 | Val Loss 1.3972 | \n",
      "Epoch 067 | Time(s) 0.4691 | Train Loss 0.9839 | Val Loss 1.4057 | \n",
      "Epoch 068 | Time(s) 0.4637 | Train Loss 0.9872 | Val Loss 1.3629 | \n",
      "Epoch 069 | Time(s) 0.4573 | Train Loss 0.9306 | Val Loss 1.3819 | \n",
      "Epoch 070 | Time(s) 0.4759 | Train Loss 0.9444 | Val Loss 1.3464 | \n",
      "Epoch 071 | Time(s) 0.4861 | Train Loss 0.9063 | Val Loss 1.3788 | \n",
      "Epoch 072 | Time(s) 0.4698 | Train Loss 0.9442 | Val Loss 1.3387 | \n",
      "Epoch 073 | Time(s) 0.4638 | Train Loss 0.9216 | Val Loss 1.3447 | \n",
      "Epoch 074 | Time(s) 0.4465 | Train Loss 0.9089 | Val Loss 1.3334 | \n",
      "Epoch 075 | Time(s) 0.4541 | Train Loss 0.8560 | Val Loss 1.3279 | \n",
      "Epoch 076 | Time(s) 0.4414 | Train Loss 0.8646 | Val Loss 1.3066 | \n",
      "Epoch 077 | Time(s) 0.4276 | Train Loss 0.8242 | Val Loss 1.2843 | \n",
      "Epoch 078 | Time(s) 0.4581 | Train Loss 0.8615 | Val Loss 1.2994 | \n",
      "Epoch 079 | Time(s) 0.4672 | Train Loss 0.8499 | Val Loss 1.2614 | \n",
      "Epoch 080 | Time(s) 0.4326 | Train Loss 0.7850 | Val Loss 1.2979 | \n",
      "Epoch 081 | Time(s) 0.4281 | Train Loss 0.8137 | Val Loss 1.2549 | \n",
      "Epoch 082 | Time(s) 0.4357 | Train Loss 0.8137 | Val Loss 1.2686 | \n",
      "Epoch 083 | Time(s) 0.4360 | Train Loss 0.7895 | Val Loss 1.2463 | \n",
      "Epoch 084 | Time(s) 0.4248 | Train Loss 0.6974 | Val Loss 1.1942 | \n",
      "Epoch 085 | Time(s) 0.4343 | Train Loss 0.7676 | Val Loss 1.2389 | \n",
      "Epoch 086 | Time(s) 0.4479 | Train Loss 0.7612 | Val Loss 1.2313 | \n",
      "Epoch 087 | Time(s) 0.4603 | Train Loss 0.7734 | Val Loss 1.2120 | \n",
      "Epoch 088 | Time(s) 0.4139 | Train Loss 0.7058 | Val Loss 1.1968 | \n",
      "Epoch 089 | Time(s) 0.4333 | Train Loss 0.7989 | Val Loss 1.2420 | \n",
      "Epoch 090 | Time(s) 0.4044 | Train Loss 0.6723 | Val Loss 1.2012 | \n",
      "Epoch 091 | Time(s) 0.4232 | Train Loss 0.7212 | Val Loss 1.1953 | \n",
      "Epoch 092 | Time(s) 0.4110 | Train Loss 0.7109 | Val Loss 1.2029 | \n",
      "Epoch 093 | Time(s) 0.4693 | Train Loss 0.7021 | Val Loss 1.1897 | \n",
      "Epoch 094 | Time(s) 0.4667 | Train Loss 0.7191 | Val Loss 1.1784 | \n",
      "Epoch 095 | Time(s) 0.4609 | Train Loss 0.6737 | Val Loss 1.1548 | \n",
      "Epoch 096 | Time(s) 0.4428 | Train Loss 0.6515 | Val Loss 1.1775 | \n",
      "Epoch 097 | Time(s) 0.5144 | Train Loss 0.6649 | Val Loss 1.1385 | \n",
      "Epoch 098 | Time(s) 0.4774 | Train Loss 0.6594 | Val Loss 1.1437 | \n",
      "Epoch 099 | Time(s) 0.4847 | Train Loss 0.6687 | Val Loss 1.1679 | \n",
      "Epoch 100 | Time(s) 0.4775 | Train Loss 0.6668 | Val Loss 1.1513 | \n",
      "Epoch 101 | Time(s) 0.4777 | Train Loss 0.6546 | Val Loss 1.1482 | \n",
      "Epoch 102 | Time(s) 0.4793 | Train Loss 0.6157 | Val Loss 1.1618 | \n",
      "Epoch 103 | Time(s) 0.4953 | Train Loss 0.6040 | Val Loss 1.1661 | \n",
      "Epoch 104 | Time(s) 0.4374 | Train Loss 0.6339 | Val Loss 1.1364 | \n",
      "Epoch 105 | Time(s) 0.4368 | Train Loss 0.6545 | Val Loss 1.1528 | \n",
      "Epoch 106 | Time(s) 0.4452 | Train Loss 0.6291 | Val Loss 1.1152 | \n",
      "Epoch 107 | Time(s) 0.4413 | Train Loss 0.6096 | Val Loss 1.1679 | \n",
      "Epoch 108 | Time(s) 0.4630 | Train Loss 0.5845 | Val Loss 1.1352 | \n",
      "Epoch 109 | Time(s) 0.4582 | Train Loss 0.6459 | Val Loss 1.1234 | \n",
      "Epoch 110 | Time(s) 0.4392 | Train Loss 0.5792 | Val Loss 1.1136 | \n",
      "Epoch 111 | Time(s) 0.4454 | Train Loss 0.5341 | Val Loss 1.1091 | \n",
      "Epoch 112 | Time(s) 0.4786 | Train Loss 0.6297 | Val Loss 1.1325 | \n",
      "Epoch 113 | Time(s) 0.4581 | Train Loss 0.5784 | Val Loss 1.1626 | \n",
      "Epoch 114 | Time(s) 0.4423 | Train Loss 0.5974 | Val Loss 1.1051 | \n",
      "Epoch 115 | Time(s) 0.4453 | Train Loss 0.5920 | Val Loss 1.1190 | \n",
      "Epoch 116 | Time(s) 0.4425 | Train Loss 0.5570 | Val Loss 1.1296 | \n",
      "Epoch 117 | Time(s) 0.4622 | Train Loss 0.5401 | Val Loss 1.1458 | \n",
      "Epoch 118 | Time(s) 0.4694 | Train Loss 0.5680 | Val Loss 1.1151 | \n",
      "Epoch 119 | Time(s) 0.5274 | Train Loss 0.5115 | Val Loss 1.1151 | \n",
      "Epoch 120 | Time(s) 0.5057 | Train Loss 0.6074 | Val Loss 1.0763 | \n",
      "Epoch 121 | Time(s) 0.5045 | Train Loss 0.5488 | Val Loss 1.1194 | \n",
      "Epoch 122 | Time(s) 0.5208 | Train Loss 0.5796 | Val Loss 1.1298 | \n",
      "Epoch 123 | Time(s) 0.4688 | Train Loss 0.5105 | Val Loss 1.0874 | \n",
      "Epoch 124 | Time(s) 0.4266 | Train Loss 0.5899 | Val Loss 1.0997 | \n",
      "Epoch 125 | Time(s) 0.4379 | Train Loss 0.4943 | Val Loss 1.0883 | \n",
      "Epoch 126 | Time(s) 0.4645 | Train Loss 0.5083 | Val Loss 1.1232 | \n",
      "Epoch 127 | Time(s) 0.5089 | Train Loss 0.5346 | Val Loss 1.0737 | \n",
      "Epoch 128 | Time(s) 0.4270 | Train Loss 0.5353 | Val Loss 1.1146 | \n",
      "Epoch 129 | Time(s) 0.4219 | Train Loss 0.5257 | Val Loss 1.1066 | \n",
      "Epoch 130 | Time(s) 0.4011 | Train Loss 0.5348 | Val Loss 1.0828 | \n",
      "Epoch 131 | Time(s) 0.4645 | Train Loss 0.5214 | Val Loss 1.0830 | \n",
      "Epoch 132 | Time(s) 0.4522 | Train Loss 0.5065 | Val Loss 1.1181 | \n",
      "Epoch 133 | Time(s) 0.4416 | Train Loss 0.5201 | Val Loss 1.1089 | \n",
      "Epoch 134 | Time(s) 0.4072 | Train Loss 0.5278 | Val Loss 1.0702 | \n",
      "Epoch 135 | Time(s) 0.4172 | Train Loss 0.5017 | Val Loss 1.0695 | \n",
      "Epoch 136 | Time(s) 0.4509 | Train Loss 0.4763 | Val Loss 1.0650 | \n",
      "Epoch 137 | Time(s) 0.4521 | Train Loss 0.5393 | Val Loss 1.0893 | \n",
      "Epoch 138 | Time(s) 0.5551 | Train Loss 0.4804 | Val Loss 1.0704 | \n",
      "Epoch 139 | Time(s) 0.4863 | Train Loss 0.4807 | Val Loss 1.1042 | \n",
      "Epoch 140 | Time(s) 0.5203 | Train Loss 0.4867 | Val Loss 1.0747 | \n",
      "Epoch 141 | Time(s) 0.4498 | Train Loss 0.5136 | Val Loss 1.1169 | \n",
      "Epoch 142 | Time(s) 0.4677 | Train Loss 0.5105 | Val Loss 1.0562 | \n",
      "Epoch 143 | Time(s) 0.4502 | Train Loss 0.4770 | Val Loss 1.0252 | \n",
      "Epoch 144 | Time(s) 0.4317 | Train Loss 0.4625 | Val Loss 1.0605 | \n",
      "Epoch 145 | Time(s) 0.4532 | Train Loss 0.4841 | Val Loss 1.0490 | \n",
      "Epoch 146 | Time(s) 0.4517 | Train Loss 0.4564 | Val Loss 1.0247 | \n",
      "Epoch 147 | Time(s) 0.4270 | Train Loss 0.4723 | Val Loss 1.0783 | \n",
      "Epoch 148 | Time(s) 0.3977 | Train Loss 0.4674 | Val Loss 1.0837 | \n",
      "Epoch 149 | Time(s) 0.4288 | Train Loss 0.4626 | Val Loss 1.0372 | \n",
      "Epoch 150 | Time(s) 0.4004 | Train Loss 0.4180 | Val Loss 0.9798 | \n",
      "Epoch 151 | Time(s) 0.3981 | Train Loss 0.4720 | Val Loss 1.0106 | \n",
      "Epoch 152 | Time(s) 0.4316 | Train Loss 0.4463 | Val Loss 1.0017 | \n",
      "Epoch 153 | Time(s) 0.4088 | Train Loss 0.5108 | Val Loss 1.0218 | \n",
      "Epoch 154 | Time(s) 0.4368 | Train Loss 0.4370 | Val Loss 1.0301 | \n",
      "Epoch 155 | Time(s) 0.4308 | Train Loss 0.4288 | Val Loss 1.0279 | \n",
      "Epoch 156 | Time(s) 0.4089 | Train Loss 0.4419 | Val Loss 1.0295 | \n",
      "Epoch 157 | Time(s) 0.4005 | Train Loss 0.4626 | Val Loss 1.0713 | \n",
      "Epoch 158 | Time(s) 0.4016 | Train Loss 0.4349 | Val Loss 1.0420 | \n",
      "Epoch 159 | Time(s) 0.4007 | Train Loss 0.4786 | Val Loss 1.0472 | \n",
      "Early stopping (val loss does not decrease for 10 consecutive epochs).\n",
      "Test Accuracy 0.8160\n"
     ]
    }
   ],
   "source": [
    "dataset = CoraGraphDataset()\n",
    "main(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2410969-ea44-4095-823a-42da24f2802f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 19717\n",
      "  NumEdges: 88651\n",
      "  NumFeats: 500\n",
      "  NumClasses: 3\n",
      "  NumTrainingSamples: 60\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n",
      "Epoch 000 | Time(s) 2.0052 | Train Loss 1.0988 | Val Loss 1.0975 | \n",
      "Epoch 001 | Time(s) 1.8083 | Train Loss 1.0966 | Val Loss 1.0962 | \n",
      "Epoch 002 | Time(s) 1.7889 | Train Loss 1.0941 | Val Loss 1.0945 | \n",
      "Epoch 003 | Time(s) 1.7796 | Train Loss 1.0912 | Val Loss 1.0927 | \n",
      "Epoch 004 | Time(s) 1.7522 | Train Loss 1.0876 | Val Loss 1.0907 | \n",
      "Epoch 005 | Time(s) 1.7654 | Train Loss 1.0836 | Val Loss 1.0878 | \n",
      "Epoch 006 | Time(s) 1.7825 | Train Loss 1.0802 | Val Loss 1.0852 | \n",
      "Epoch 007 | Time(s) 1.9341 | Train Loss 1.0741 | Val Loss 1.0829 | \n",
      "Epoch 008 | Time(s) 1.9418 | Train Loss 1.0699 | Val Loss 1.0784 | \n",
      "Epoch 009 | Time(s) 1.8770 | Train Loss 1.0641 | Val Loss 1.0763 | \n",
      "Epoch 010 | Time(s) 1.7873 | Train Loss 1.0570 | Val Loss 1.0731 | \n",
      "Epoch 011 | Time(s) 1.8458 | Train Loss 1.0499 | Val Loss 1.0684 | \n",
      "Epoch 012 | Time(s) 1.8477 | Train Loss 1.0453 | Val Loss 1.0649 | \n",
      "Epoch 013 | Time(s) 1.9639 | Train Loss 1.0296 | Val Loss 1.0617 | \n",
      "Epoch 014 | Time(s) 1.8510 | Train Loss 1.0271 | Val Loss 1.0589 | \n",
      "Epoch 015 | Time(s) 1.9405 | Train Loss 1.0158 | Val Loss 1.0517 | \n",
      "Epoch 016 | Time(s) 2.1573 | Train Loss 1.0131 | Val Loss 1.0475 | \n",
      "Epoch 017 | Time(s) 2.1113 | Train Loss 1.0008 | Val Loss 1.0398 | \n",
      "Epoch 018 | Time(s) 1.8745 | Train Loss 0.9894 | Val Loss 1.0361 | \n",
      "Epoch 019 | Time(s) 1.9283 | Train Loss 0.9776 | Val Loss 1.0286 | \n",
      "Epoch 020 | Time(s) 2.1011 | Train Loss 0.9683 | Val Loss 1.0246 | \n",
      "Epoch 021 | Time(s) 1.8427 | Train Loss 0.9459 | Val Loss 1.0185 | \n",
      "Epoch 022 | Time(s) 1.9281 | Train Loss 0.9402 | Val Loss 1.0121 | \n",
      "Epoch 023 | Time(s) 1.7722 | Train Loss 0.9370 | Val Loss 1.0054 | \n",
      "Epoch 024 | Time(s) 1.7774 | Train Loss 0.9230 | Val Loss 0.9991 | \n",
      "Epoch 025 | Time(s) 1.8678 | Train Loss 0.9092 | Val Loss 0.9897 | \n",
      "Epoch 026 | Time(s) 2.1127 | Train Loss 0.8925 | Val Loss 0.9844 | \n",
      "Epoch 027 | Time(s) 2.1571 | Train Loss 0.8857 | Val Loss 0.9787 | \n",
      "Epoch 028 | Time(s) 2.1304 | Train Loss 0.8685 | Val Loss 0.9672 | \n",
      "Epoch 029 | Time(s) 2.0761 | Train Loss 0.8550 | Val Loss 0.9567 | \n",
      "Epoch 030 | Time(s) 2.0175 | Train Loss 0.8239 | Val Loss 0.9450 | \n",
      "Epoch 031 | Time(s) 2.0843 | Train Loss 0.8228 | Val Loss 0.9477 | \n",
      "Epoch 032 | Time(s) 1.8750 | Train Loss 0.8054 | Val Loss 0.9418 | \n",
      "Epoch 033 | Time(s) 2.0581 | Train Loss 0.7940 | Val Loss 0.9309 | \n",
      "Epoch 034 | Time(s) 2.0703 | Train Loss 0.7730 | Val Loss 0.9188 | \n",
      "Epoch 035 | Time(s) 2.0815 | Train Loss 0.7752 | Val Loss 0.9147 | \n",
      "Epoch 036 | Time(s) 2.1350 | Train Loss 0.7454 | Val Loss 0.9105 | \n",
      "Epoch 037 | Time(s) 2.1525 | Train Loss 0.7497 | Val Loss 0.9067 | \n",
      "Epoch 038 | Time(s) 1.9780 | Train Loss 0.7442 | Val Loss 0.8841 | \n",
      "Epoch 039 | Time(s) 1.8281 | Train Loss 0.7374 | Val Loss 0.8833 | \n",
      "Epoch 040 | Time(s) 2.0308 | Train Loss 0.6750 | Val Loss 0.8804 | \n",
      "Epoch 041 | Time(s) 1.8804 | Train Loss 0.6960 | Val Loss 0.8663 | \n",
      "Epoch 042 | Time(s) 1.9609 | Train Loss 0.6648 | Val Loss 0.8602 | \n",
      "Epoch 043 | Time(s) 2.1424 | Train Loss 0.6679 | Val Loss 0.8458 | \n",
      "Epoch 044 | Time(s) 2.0937 | Train Loss 0.6503 | Val Loss 0.8534 | \n",
      "Epoch 045 | Time(s) 2.0832 | Train Loss 0.5959 | Val Loss 0.8339 | \n",
      "Epoch 046 | Time(s) 2.0242 | Train Loss 0.6158 | Val Loss 0.8218 | \n",
      "Epoch 047 | Time(s) 2.1211 | Train Loss 0.6158 | Val Loss 0.8261 | \n",
      "Epoch 048 | Time(s) 1.9115 | Train Loss 0.5780 | Val Loss 0.8192 | \n",
      "Epoch 049 | Time(s) 2.0921 | Train Loss 0.5891 | Val Loss 0.8172 | \n",
      "Epoch 050 | Time(s) 2.0412 | Train Loss 0.5902 | Val Loss 0.7900 | \n",
      "Epoch 051 | Time(s) 2.1336 | Train Loss 0.5608 | Val Loss 0.7980 | \n",
      "Epoch 052 | Time(s) 2.0601 | Train Loss 0.5642 | Val Loss 0.7900 | \n",
      "Epoch 053 | Time(s) 2.1060 | Train Loss 0.5786 | Val Loss 0.7803 | \n",
      "Epoch 054 | Time(s) 2.0953 | Train Loss 0.5305 | Val Loss 0.7822 | \n",
      "Epoch 055 | Time(s) 2.0375 | Train Loss 0.5307 | Val Loss 0.7695 | \n",
      "Epoch 056 | Time(s) 2.0089 | Train Loss 0.5302 | Val Loss 0.7647 | \n",
      "Epoch 057 | Time(s) 2.0940 | Train Loss 0.5092 | Val Loss 0.7797 | \n",
      "Epoch 058 | Time(s) 2.1006 | Train Loss 0.5128 | Val Loss 0.7629 | \n",
      "Epoch 059 | Time(s) 2.0521 | Train Loss 0.4544 | Val Loss 0.7562 | \n",
      "Epoch 060 | Time(s) 1.9717 | Train Loss 0.4670 | Val Loss 0.7628 | \n",
      "Epoch 061 | Time(s) 2.0411 | Train Loss 0.4689 | Val Loss 0.7345 | \n",
      "Epoch 062 | Time(s) 1.9755 | Train Loss 0.4669 | Val Loss 0.7449 | \n",
      "Epoch 063 | Time(s) 1.8623 | Train Loss 0.4744 | Val Loss 0.7332 | \n",
      "Epoch 064 | Time(s) 1.9546 | Train Loss 0.4519 | Val Loss 0.7202 | \n",
      "Epoch 065 | Time(s) 2.1959 | Train Loss 0.4425 | Val Loss 0.7203 | \n",
      "Epoch 066 | Time(s) 1.8804 | Train Loss 0.4451 | Val Loss 0.7228 | \n",
      "Epoch 067 | Time(s) 1.8022 | Train Loss 0.4332 | Val Loss 0.7392 | \n",
      "Epoch 068 | Time(s) 2.1260 | Train Loss 0.4486 | Val Loss 0.7193 | \n",
      "Epoch 069 | Time(s) 2.1480 | Train Loss 0.4029 | Val Loss 0.7013 | \n",
      "Epoch 070 | Time(s) 1.7970 | Train Loss 0.3923 | Val Loss 0.6977 | \n",
      "Epoch 071 | Time(s) 2.0170 | Train Loss 0.3954 | Val Loss 0.7263 | \n",
      "Epoch 072 | Time(s) 2.0144 | Train Loss 0.3762 | Val Loss 0.7163 | \n",
      "Epoch 073 | Time(s) 1.9707 | Train Loss 0.3860 | Val Loss 0.6908 | \n",
      "Epoch 074 | Time(s) 2.0066 | Train Loss 0.3719 | Val Loss 0.6998 | \n",
      "Epoch 075 | Time(s) 2.1801 | Train Loss 0.3776 | Val Loss 0.6908 | \n",
      "Epoch 076 | Time(s) 2.1400 | Train Loss 0.3543 | Val Loss 0.6954 | \n",
      "Epoch 077 | Time(s) 2.0403 | Train Loss 0.3369 | Val Loss 0.6944 | \n",
      "Epoch 078 | Time(s) 2.1349 | Train Loss 0.3441 | Val Loss 0.6909 | \n",
      "Epoch 079 | Time(s) 2.0181 | Train Loss 0.3499 | Val Loss 0.6936 | \n",
      "Epoch 080 | Time(s) 2.0916 | Train Loss 0.3529 | Val Loss 0.6899 | \n",
      "Epoch 081 | Time(s) 2.0111 | Train Loss 0.3468 | Val Loss 0.6754 | \n",
      "Epoch 082 | Time(s) 1.9043 | Train Loss 0.3348 | Val Loss 0.6839 | \n",
      "Epoch 083 | Time(s) 2.0724 | Train Loss 0.3278 | Val Loss 0.6796 | \n",
      "Epoch 084 | Time(s) 2.1032 | Train Loss 0.3344 | Val Loss 0.6817 | \n",
      "Epoch 085 | Time(s) 2.0725 | Train Loss 0.3399 | Val Loss 0.6678 | \n",
      "Epoch 086 | Time(s) 2.1561 | Train Loss 0.2661 | Val Loss 0.6859 | \n",
      "Epoch 087 | Time(s) 2.1572 | Train Loss 0.2824 | Val Loss 0.6721 | \n",
      "Epoch 088 | Time(s) 2.1399 | Train Loss 0.3340 | Val Loss 0.6648 | \n",
      "Epoch 089 | Time(s) 2.1292 | Train Loss 0.3169 | Val Loss 0.6802 | \n",
      "Epoch 090 | Time(s) 2.0463 | Train Loss 0.2853 | Val Loss 0.6716 | \n",
      "Epoch 091 | Time(s) 2.0598 | Train Loss 0.3081 | Val Loss 0.6830 | \n",
      "Epoch 092 | Time(s) 2.1338 | Train Loss 0.3090 | Val Loss 0.6812 | \n",
      "Epoch 093 | Time(s) 2.1779 | Train Loss 0.3221 | Val Loss 0.6721 | \n",
      "Epoch 094 | Time(s) 2.1525 | Train Loss 0.2969 | Val Loss 0.6664 | \n",
      "Epoch 095 | Time(s) 2.0838 | Train Loss 0.3371 | Val Loss 0.6567 | \n",
      "Epoch 096 | Time(s) 2.0153 | Train Loss 0.2848 | Val Loss 0.6576 | \n",
      "Epoch 097 | Time(s) 2.0547 | Train Loss 0.2826 | Val Loss 0.6597 | \n",
      "Epoch 098 | Time(s) 2.0734 | Train Loss 0.2824 | Val Loss 0.6522 | \n",
      "Epoch 099 | Time(s) 2.0887 | Train Loss 0.3038 | Val Loss 0.6557 | \n",
      "Epoch 100 | Time(s) 2.1373 | Train Loss 0.3033 | Val Loss 0.6579 | \n",
      "Epoch 101 | Time(s) 2.0195 | Train Loss 0.2836 | Val Loss 0.6524 | \n",
      "Epoch 102 | Time(s) 2.1972 | Train Loss 0.2818 | Val Loss 0.6804 | \n",
      "Epoch 103 | Time(s) 2.0550 | Train Loss 0.2818 | Val Loss 0.6606 | \n",
      "Epoch 104 | Time(s) 1.8085 | Train Loss 0.2611 | Val Loss 0.6748 | \n",
      "Epoch 105 | Time(s) 1.7705 | Train Loss 0.2463 | Val Loss 0.6641 | \n",
      "Epoch 106 | Time(s) 1.8367 | Train Loss 0.2530 | Val Loss 0.6756 | \n",
      "Epoch 107 | Time(s) 2.1038 | Train Loss 0.2975 | Val Loss 0.6524 | \n",
      "Early stopping (val loss does not decrease for 10 consecutive epochs).\n",
      "Test Accuracy 0.7890\n"
     ]
    }
   ],
   "source": [
    "dataset = PubmedGraphDataset()\n",
    "main(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d515aa2-9b46-4552-9ba0-14a17f8727d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
